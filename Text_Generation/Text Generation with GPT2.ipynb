{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsingh9076/GPT/blob/main/Text_Generation/Text%20Generation%20with%20GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g60czY19wIuw"
      },
      "source": [
        "<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project08%20-%20Text%20Generation%20with%20Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLB25msp3Xen"
      },
      "source": [
        "# Text Generation with Transformers\n",
        "\n",
        "It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.\n",
        "\n",
        "Here we will use the GPT-2 Model to generate text based on an input sequence of text.\n",
        "\n",
        "![](https://i.imgur.com/z4k1IzU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5yq8CrH4ibV"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "9Ba8d9r3wbOI",
        "outputId": "4122a74f-7a95-48d6-9c49-beafc6b0bd13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n",
            "\r",
            "\u001b[K     |██▍                             | 10kB 20.7MB/s eta 0:00:01\r",
            "\u001b[K     |████▊                           | 20kB 6.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████▏                        | 30kB 9.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▌                      | 40kB 5.8MB/s eta 0:00:01\r",
            "\u001b[K     |████████████                    | 51kB 7.1MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▎                 | 61kB 8.4MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▊               | 71kB 9.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████             | 81kB 10.6MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▌          | 92kB 11.7MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▉        | 102kB 9.4MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▎     | 112kB 9.4MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▋   | 122kB 9.4MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████ | 133kB 9.4MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████████| 143kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
            "Collecting sentencepiece (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r",
            "\u001b[K     |▎                               | 10kB 23.8MB/s eta 0:00:01\r",
            "\u001b[K     |▋                               | 20kB 32.4MB/s eta 0:00:01\r",
            "\u001b[K     |█                               | 30kB 38.5MB/s eta 0:00:01\r",
            "\u001b[K     |█▎                              | 40kB 41.5MB/s eta 0:00:01\r",
            "\u001b[K     |█▋                              | 51kB 44.0MB/s eta 0:00:01\r",
            "\u001b[K     |██                              | 61kB 47.4MB/s eta 0:00:01\r",
            "\u001b[K     |██▏                             | 71kB 50.6MB/s eta 0:00:01\r",
            "\u001b[K     |██▌                             | 81kB 51.3MB/s eta 0:00:01\r",
            "\u001b[K     |██▉                             | 92kB 53.5MB/s eta 0:00:01\r",
            "\u001b[K     |███▏                            | 102kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███▌                            | 112kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███▉                            | 122kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████                            | 133kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████▍                           | 143kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████▊                           | 153kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████                           | 163kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████▍                          | 174kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████▊                          | 184kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████                          | 194kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████▎                         | 204kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████▋                         | 215kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████                         | 225kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████▎                        | 235kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████▋                        | 245kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████▉                        | 256kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████▏                       | 266kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████▌                       | 276kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████▉                       | 286kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▏                      | 296kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▌                      | 307kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▊                      | 317kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████                      | 327kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▍                     | 337kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▊                     | 348kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████                     | 358kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████▍                    | 368kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████▋                    | 378kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████                    | 389kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▎                   | 399kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▋                   | 409kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████                   | 419kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▎                  | 430kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▌                  | 440kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▉                  | 450kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▏                 | 460kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▌                 | 471kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▉                 | 481kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▏                | 491kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▍                | 501kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▊                | 512kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████                | 522kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▍               | 532kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▊               | 542kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████               | 552kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▎              | 563kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▋              | 573kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████              | 583kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████▎             | 593kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████▋             | 604kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████             | 614kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▎            | 624kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▌            | 634kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▉            | 645kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▏           | 655kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▌           | 665kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▉           | 675kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▏          | 686kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▍          | 696kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▊          | 706kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████          | 716kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████▍         | 727kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████▊         | 737kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████         | 747kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▎        | 757kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▋        | 768kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████        | 778kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▎       | 788kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▋       | 798kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████       | 808kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▏      | 819kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▌      | 829kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▉      | 839kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▏     | 849kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▌     | 860kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▉     | 870kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████     | 880kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████▍    | 890kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████▊    | 901kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████    | 911kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▍   | 921kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▊   | 931kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████   | 942kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████▎  | 952kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████▋  | 962kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████  | 972kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▎ | 983kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▋ | 993kB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▉ | 1.0MB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▏| 1.0MB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▌| 1.0MB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▉| 1.0MB 55.5MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████████| 1.0MB 55.5MB/s \n",
            "\u001b[?25hCollecting regex (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 63.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.199)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.199 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.199)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.6.8-cp36-cp36m-linux_x86_64.whl size=604147 sha256=d699073966ed82702f676a7257d316709a31062de5634202b97d1c12c38f5507\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: sentencepiece, regex, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.0.0 regex-2019.6.8 sentencepiece-0.1.82\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvmCi8wt4k5G"
      },
      "source": [
        "# Load GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvU2C3WUwrJa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "aME-hHZjwwNT",
        "outputId": "78223fa8-1692-4d8a-d31d-4dfe45379e3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 2064188.98B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1331602.62B/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRYKn1dA4pBk"
      },
      "source": [
        "# Next Word Generation with GPT-2\n",
        "\n",
        "GPT-2 is a successor of GPT, the original NLP framework by OpenAI. The full GPT-2 model has 1.5 billion parameters, which is almost 10 times the parameters of GPT. GPT-2 give State-of-the Art results as you might have surmised already (and will soon see when we get into Python).\n",
        "\n",
        "The pre-trained model contains data from 8 million web pages collected from outbound links from Reddit.\n",
        "\n",
        "![](https://i.imgur.com/TbnGbjX.png)\n",
        "\n",
        "The architecture of GPT-2 is based on the very famous Transformers concept that was proposed by Google in their paper “Attention is all you need”. The Transformer provides a mechanism based on encoder-decoders to detect input-output dependencies.\n",
        "\n",
        "At each step, the model consumes the previously generated symbols as additional input when generating the next output.\n",
        "\n",
        "![](https://i.imgur.com/0XSSXBd.png)\n",
        "\n",
        "Modifications in GPT-2 include:\n",
        "\n",
        "- The model uses larger context and vocabulary size\n",
        "- After the final self-attention block, an additional normalization layer is added\n",
        "- Similar to a residual unit of type “building block”, layer normalization is moved to the input of each sub-block. It has batch normalization applied before weight layers, which is different from the original type “bottleneck”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7t3HtgUkw0pX",
        "outputId": "91fa611c-0ce4-4c86-eed8-1e1255c6523d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[14618, 284, 262, 1280, 1366, 3783, 4495, 340, 318]"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Welcome to the open data science conference it is\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "indexed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EINvRPSzxDxP",
        "outputId": "299e9a70-6c87-4b9c-ce1d-a6ce326c596c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[14618,   284,   262,  1280,  1366,  3783,  4495,   340,   318]])"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "tokens_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eK3vlMWmxJpH",
        "outputId": "483782eb-9c42-4341-d3e9-ba0c2c4cd9c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 176/176 [00:00<00:00, 43316.37B/s]\n",
            "100%|██████████| 548118077/548118077 [00:18<00:00, 29635831.08B/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y5w_1JK1xOy2",
        "outputId": "d504cbd8-144f-4bbc-b577-e9ce361fd1e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W2842GKxZ23"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1BHGaAVLxffT",
        "outputId": "9e01569d-f83d-47ad-eead-69e91c575135"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 50257])"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yxdytwGIxhxp",
        "outputId": "8b55c8a2-7aa8-42a5-81b1-4373d587e9b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Welcome to the open data science conference it is a'"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "predicted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZwNnlQmx45H"
      },
      "outputs": [],
      "source": [
        "start = 'Natural Language Processing is slowly becoming'\n",
        "indexed_tokens = tokenizer.encode(start)\n",
        "\n",
        "for i in range(75):\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "    indexed_tokens = indexed_tokens + [predicted_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "oFhvg-ouykk1",
        "outputId": "5028fa80-e5a1-401d-a34d-8bf54783ad1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Processing is slowly becoming a reality.\n",
            "\n",
            "The first step is to create a language processing system that can be used to create a language. This is done by using a language processing system that is built on top of a language processing system.\n",
            "\n",
            "The language processing system is a set of tools that can be used to create a language. The language processing system is a set of tools that can can\n"
          ]
        }
      ],
      "source": [
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "print(predicted_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text Generation with Transformers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}